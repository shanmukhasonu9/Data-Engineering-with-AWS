from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('SparkByExamples').getOrCreate()
data = [("James","Smith","USA","CA"),("Michael","Rose","USA","NY"), \
    ("Robert","Williams","USA","CA"),("Maria","Jones","USA","FL") \
  ]
columns=["firstname","lastname","country","state"]
df=spark.createDataFrame(data=data,schema=columns)
df.show()
print(df.collect())

spark-submit --deploy-mode cluster --master yarn practice4.py

states1=df.rdd.map(lambda x: x[3]).collect()
print(states1)

#spark-submit --deploy-mode cluster --master yarn practice4.py
